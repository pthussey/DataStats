"""This module contains classes for conducting various types of hypothesis tests 
using non-parametric computational methods, inspired by the methods used in Allen Downey's book Think Stats.  
These classes can produce p-values, calculate the power of the test being performed, 
and plot a CDF of the sampling distribution. 
There are also some additional functions included for chisquared and ANOVA post-hoc analysis.
"""

import numpy as np
import pandas as pd

import matplotlib
import matplotlib.pyplot as plt

import scipy.stats as stats

from collections import Counter

from itertools import combinations
from math import factorial

from functools import reduce

from .singlevar import DiscreteRv


class UnimplementedMethodException(Exception):
    """Exception if someone calls a method that should be overridden."""


class HypothesisTest():
    """Hypothesis test superclass. 

    This class cannot be used as is. 
    It is to be used to construct hypothesis tests 
    for various different test statistics. 
    See the existing child classes below for examples.
    """
    def __init__(self, data, tail='right', iters=1000):
        self.data = data
        self.tail = tail
        self.iters = iters
        self.PrepareData(data)
        self.TestStat()
        self.sampling_dist, self.rv = self.ComputeRv() # pylint: disable=assignment-from-no-return

    # Provide the functionality to convert the data into the format needed 
    # for use in ComputeRv and Power functions. 
    # Ex. Convert to array, split data into component groups, etc. 
    # The self data variables must be created in the function, not returned. 
    # See child classes for examples
    def PrepareData(self, data):
        UnimplementedMethodException()
        
    # This function only needs to be written in the case of a null hypothesis based test. 
    # The self.test_stat needs to be created in the function, not returned. 
    # In the case of an alternative hypothesis based test 
    # test_stat will be provided via a class parameter. 
    # See child classes for examples
    def TestStat(self):
        pass
    
    # Provide the functionality that computes the sampling distribution and rv for the data.
    # Both the sampling distribution and the rv need to be returned by the function 
    # See child classes for examples
    def ComputeRv(self):
        UnimplementedMethodException()
        
    # Provide the functionality that computes the power by running multiple iterations of the hypothesis test.
    # The code in the for loop must first create new data for the run, 
    # which simulates taking another sample from the population, and then run the hypothesis test.
    # See child classes for examples
    def Power(self):
        UnimplementedMethodException()
    
    def PValue(self):
        """Computes the p-value for the hypothesis test.

        returns: float p-value
        """
        if self.tail == 'left':
            pvalue = self.rv.cdf(self.test_stat) # pylint: disable=no-member
        elif self.tail == 'right':
            pvalue = 1 - self.rv.cdf(self.test_stat) # pylint: disable=no-member
        else:
            raise Exception('The value of \'tail\' can only be either \'left\' or \'right\'')

        return pvalue

    def MinMaxTestStat(self):
        """Returns the smallest and largest test statistics in the sampling distribution.
        """
        return min(self.sampling_dist), max(self.sampling_dist)

    def PlotCdf(self):
        """Draws a Cdf with a vertical line at the test stat.
        """      
        plt.plot(self.rv.xk, self.rv.cdf(self.rv.xk), color='C0', lw=2) # pylint: disable=no-member
        
        plt.axvline(self.test_stat, color='C3', lw=1.3, label='Test Stat') # pylint: disable=no-member

        plt.legend(frameon=True, bbox_to_anchor=(1.04,1), loc="upper left")

        plt.show()


class HTMean(HypothesisTest):
    """This class is used to conduct one-sample mean hypothesis testing. 
    A test_stat to represent the null hypothesis must be provided. 
    This test can only produce a onesided pvalue.

    Parameters
    ----------
    data (array-like):
        1D sequence of data
    test_stat (float):
        The test stat to represent the null hypothesis
    tail (str):
        The tail of the distribution to be used in the PValue function
        Accepts only 'right' or 'left'. Defaults to 'right'
    iters (int):
        The number of iterations to run in the ComputeRv function 
        Defaults to 1000
    
    Attributes
    ----------
    data:
        The original data
    test_stat:
        The test statistic used in the hypothesis test
    sampling_dist:
        The sampling distribution generated by resampling
    rv:
        A scipy.stats discrete_rv object (random variable) 
        that represents the sampling distribution
        This object provides numerous useful attributes and methods
        See the discrete_rv documentation for details

    Methods
    -------
    PValue():
        Computes the p-value for the hypothesis test
    Power(alpha=0.05, num_runs=1000):
        Computes the power of the hypothesis test
        alpha: the significance level for the hypothesis test, default=0.05
        num_runs: the number of hypothesis tests to run, default=1000
    MinMaxTestStat():
        Returns the smallest and largest test statistics in the sampling distribution
    PlotCdf():
        Draws a Cdf of the distribution with a vertical line at the test stat
    
    """  
    # For tests that resample to build a distribution for the alternative hypothesis 
    # a test_stat parameter must be included in __init__ to represent the null hypothesis
    # This test_stat is made an attribute of the class below
    def __init__(self, data, test_stat, tail='right', iters=1000):
        self.test_stat = test_stat
        HypothesisTest.__init__(self, data, tail, iters)

    def PrepareData(self, data):
        self.data = np.array(self.data)
        
    def ComputeRv(self):       
        # Build the sampling distribution
        mean_estimates = [np.random.choice(self.data, size=len(self.data), replace=True).
                          mean() for _ in range(self.iters)]

        return np.array(mean_estimates), DiscreteRv(mean_estimates)

    def Power(self, alpha=0.05, num_runs=1000):
        """Computes the power of the hypothesis test. 

        Args
        ----
        alpha (float):
            The significance level for the hypothesis test.
            Must be between 0 and 1. Defaults to 0.05
        num_runs (int):
            The number of times to run the hypothesis test to compute power.
            Defaults to 1000.

        Returns
        -------
        power:
            Computed as the percentage of significant pvalues in num_runs of the test.
            Returned value is between 0 and 1.
        """        
        pvalue_count = 0
        
        for _ in range(num_runs):
            # Create run_data that simulates taking another sample from the population
            run_data = np.random.choice(self.data, size=len(self.data), replace=True)
        
            # Run the hypothesis test with run_data
            test = HTMean(run_data, test_stat=self.test_stat, tail=self.tail, iters=100)
            pvalue = test.PValue()
            
            if pvalue < alpha:
                pvalue_count += 1
            
        return pvalue_count / num_runs


class HTDiffMeansH0(HypothesisTest):
    """This class is used to conduct difference of means hypothesis testing. 
    Uses permutation to build a sampling distribution that represents the null hypothesis.
    Accepts data as a list or tuple of two groups of data (eg. (group1, group2)).

    Parameters
    ----------
    data (array-like):
        A list or tuple of two groups of data (eg. (group1, group2))
    onesided (boolean):
        If set to True, a one-sided test is run
        If set to False, a two-sided test is run using abs value in statistic calculations
        Defaults to False 
    tail (str):
        The tail of the distribution to be used in the PValue function
        Accepts only 'right' or 'left'
        Defaults to 'right'
    iters (int):
        The number of iterations to run in the ComputeRv function 
        Defaults to 1000
    
    Attributes
    ----------
    data:
        The original data
    test_stat:
        The test statistic used in the hypothesis test
    sampling_dist:
        The sampling distribution generated by resampling
    rv:
        A scipy.stats discrete_rv object (random variable) 
        that represents the sampling distribution
        This object provides numerous useful attributes and methods
        See the discrete_rv documentation for details

    Methods
    -------
    PValue():
        Computes the p-value for the hypothesis test
    Power(alpha=0.05, num_runs=1000):
        Computes the power of the hypothesis test
        alpha: the significance level for the hypothesis test, default=0.05
        num_runs: the number of hypothesis tests to run, default=1000
    MinMaxTestStat():
        Returns the smallest and largest test statistics in the sampling distribution
    PlotCdf():
        Draws a Cdf of the distribution with a vertical line at the test stat
    """
    def __init__(self, data, onesided=False, tail='right', iters=1000):
        self.onesided = onesided
        HypothesisTest.__init__(self, data, tail, iters)

    def PrepareData(self, data):
        a, b = data
        self.a = np.array(a)
        self.b = np.array(b)
        
    # For tests that resample to build a distribution for the null hypothesis 
    # the calculation of the test stat must be provided. 
    # The calculation must create self.test_stat, not return the value.
    # In the case of a twosided test the absolute value of the difference in means is used
    def TestStat(self):      
        if self.onesided == False:
            self.test_stat = abs(self.a.mean() - self.b.mean())
        elif self.onesided == True:
            self.test_stat = self.a.mean() - self.b.mean()
        else:
            raise TypeError('\'onesided\' parameter only accepts Boolean True or False')
        
    def ComputeRv(self):
        # Create the pooled data
        pooled_data = np.hstack((self.a, self.b))
        # Compute the size of a, size of b is just the rest
        a_size = len(self.a)
        
        # Build the sampling distribution
        diff_mean_results = []
        
        if self.onesided == False:
            for _ in range(self.iters):
                np.random.shuffle(pooled_data)
                group1 = pooled_data[:a_size]
                group2 = pooled_data[a_size:]
                result = abs(group1.mean() - group2.mean())
                
                diff_mean_results.append(result)
                    
        elif self.onesided == True:
            for _ in range(self.iters):
                np.random.shuffle(pooled_data)
                group1 = pooled_data[:a_size]
                group2 = pooled_data[a_size:]
                result = group1.mean() - group2.mean()
                
                diff_mean_results.append(result)
                
        else:
            raise TypeError('\'onesided\' parameter only accepts Boolean True or False')
            
        return np.array(diff_mean_results), DiscreteRv(diff_mean_results)
    
    def Power(self, alpha=0.05, num_runs=1000):
        """Computes the power of the hypothesis test. 

        Args
        ----
        alpha (float):
            The significance level for the hypothesis test.
            Must be between 0 and 1. Defaults to 0.05
        num_runs (int):
            The number of times to run the hypothesis test to compute power.
            Defaults to 1000.

        Returns
        -------
        power:
            Computed as the percentage of significant pvalues in num_runs of the test.
            Returned value is between 0 and 1.
        """      
        pvalue_count = 0
        
        for _ in range(num_runs):
            # Create run_data that simulates taking another sample from the population
            sample1 = np.random.choice(self.a, size=len(self.a), replace=True)
            sample2 = np.random.choice(self.b, size=len(self.b), replace=True)
            run_data = sample1, sample2
        
            # Run the hypothesis test with run_data
            test = HTDiffMeansH0(run_data, onesided=self.onesided, tail=self.tail, iters=100)
            pvalue = test.PValue()
            
            if pvalue < alpha:
                pvalue_count += 1
            
        return pvalue_count / num_runs


class HTDiffMeansHa(HypothesisTest):
    """This class is used to conduct difference of means hypothesis testing. 
    Uses resampling to build a sampling distribution that represents the alternative hypothesis.
    A test_stat to represent the null hypothesis must be provided. 
    This test can only produce a onesided pvalue.
    Accepts data as a list or tuple of two groups of data (eg. (group1, group2)).

    Parameters
    ----------
    data (array-like):
        A list or tuple of two groups of data (eg. (group1, group2))
    test_stat (float):
        The test stat to represent the null hypothesis
    tail (str):
        The tail of the distribution to be used in the PValue function
        Accepts only 'right' or 'left'
        Defaults to 'right'
    iters (int):
        The number of iterations to run in the ComputeRv function 
        Defaults to 1000
    
    Attributes
    ----------
    data:
        The original data
    test_stat:
        The test statistic used in the hypothesis test
    sampling_dist:
        The sampling distribution generated by resampling
    rv:
        A scipy.stats discrete_rv object (random variable) 
        that represents the sampling distribution
        This object provides numerous useful attributes and methods
        See the discrete_rv documentation for details

    Methods
    -------
    PValue():
        Computes the p-value for the hypothesis test
    Power(alpha=0.05, num_runs=1000):
        Computes the power of the hypothesis test
        alpha: the significance level for the hypothesis test, default=0.05
        num_runs: the number of hypothesis tests to run, default=1000
    MinMaxTestStat():
        Returns the smallest and largest test statistics in the sampling distribution
    PlotCdf():
        Draws a Cdf of the distribution with a vertical line at the test stat
    """
    # For tests that resample to build a distribution for the alternative hypothesis 
    # a test_stat parameter must be included in __init__ to represent the null hypothesis
    # This test_stat is made an attribute of the class below
    def __init__(self, data, test_stat, tail='right', iters=1000):
        self.test_stat = test_stat
        HypothesisTest.__init__(self, data, tail, iters)

    def PrepareData(self, data):
        a, b = data
        self.a = np.array(a)
        self.b = np.array(b)

    def ComputeRv(self):      
        # Build the sampling distribution
        diff_mean_results = []
        
        for _ in range(self.iters):
            group1 = np.random.choice(self.a, size=len(self.a), replace=True)
            group2 = np.random.choice(self.b, size=len(self.b), replace=True)
            result = group1.mean() - group2.mean()
                
            diff_mean_results.append(result)
           
        return np.array(diff_mean_results), DiscreteRv(diff_mean_results)
    
    def Power(self, alpha=0.05, num_runs=1000):
        """Computes the power of the hypothesis test. 

        Args
        ----
        alpha (float):
            The significance level for the hypothesis test.
            Must be between 0 and 1. Defaults to 0.05
        num_runs (int):
            The number of times to run the hypothesis test to compute power.
            Defaults to 1000.

        Returns
        -------
        power:
            Computed as the percentage of significant pvalues in num_runs of the test.
            Returned value is between 0 and 1.
        """      
        pvalue_count = 0
        
        for _ in range(num_runs):
            # Create run data
            sample1 = np.random.choice(self.a, size=len(self.a), replace=True)
            sample2 = np.random.choice(self.b, size=len(self.b), replace=True)
            run_data = sample1, sample2
        
            # Run the hypothesis test with run_data
            test = HTDiffMeansHa(run_data, test_stat=self.test_stat, tail=self.tail, iters=100)
            pvalue = test.PValue()
            
            if pvalue < alpha:
                pvalue_count += 1
            
        return pvalue_count / num_runs


class HTCorrelationH0(HypothesisTest):
    """This class is used to conduct correlation hypothesis testing. 
    Uses permutation to build a sampling distribution that represents the null hypothesis.
    Accepts data as a list or tuple of two groups of data (eg. (x, y))

    Parameters
    ----------
    data (array-like):
        A list or tuple of two groups of data (eg. (x, y))
    onesided (boolean):
        If set to True, a one-sided test is run
        If set to False, a two-sided test is run using abs value in statistic calculations
        Defaults to False 
    tail (str):
        The tail of the distribution to be used in the PValue function
        Accepts only 'right' or 'left'
        Defaults to 'right'
    iters (int):
        The number of iterations to run in the ComputeRv function 
        Defaults to 1000
    method (str):
        The method to use for correlation calcluation.
        Accepts only 'pearson' or 'spearman'
        Defaults to 'pearson
    
    Attributes
    ----------
    data:
        The original data
    test_stat:
        The test statistic used in the hypothesis test
    sampling_dist:
        The sampling distribution generated by resampling
    rv:
        A scipy.stats discrete_rv object (random variable) 
        that represents the sampling distribution
        This object provides numerous useful attributes and methods
        See the discrete_rv documentation for details

    Methods
    -------
    PValue():
        Computes the p-value for the hypothesis test
    Power(alpha=0.05, num_runs=1000):
        Computes the power of the hypothesis test
        alpha: the significance level for the hypothesis test, default=0.05
        num_runs: the number of hypothesis tests to run, default=1000
    MinMaxTestStat():
        Returns the smallest and largest test statistics in the sampling distribution
    PlotCdf():
        Draws a Cdf of the distribution with a vertical line at the test stat
    """
    def __init__(self, data, onesided=False, tail='right', iters=1000, method='pearson'):
        self.onesided = onesided
        self.method = method
        HypothesisTest.__init__(self, data, tail, iters)

    def PrepareData(self, data):
        x, y = data
        self.x = np.array(x)
        self.y = np.array(y)
        
    # For tests that resample to build a distribution for the null hypothesis 
    # the calculation of the test stat must be provided.
    # In the case of a twosided test the absolute value of the difference in means is used
    def TestStat(self):
        if (self.onesided == False) & (self.method == 'pearson'):
            self.test_stat = abs(stats.pearsonr(self.x , self.y)[0])
        elif (self.onesided == False) & (self.method == 'spearman'):
            self.test_stat = abs(stats.spearmanr(self.x , self.y)[0])
        elif (self.onesided == True) & (self.method == 'pearson'):
            self.test_stat = stats.pearsonr(self.x , self.y)[0]
        elif (self.onesided == True) & (self.method == 'spearman'):
            self.test_stat = stats.spearmanr(self.x , self.y)[0]
        else:
            raise ValueError('\'onesided\' parameter only accepts Boolean True or False, ' +
                             'and \'method\' only accepts \'pearson\' or \'spearman\'')
        
    def ComputeRv(self):
        # Build the sampling distribution
        corrs = []
        
        if (self.onesided == False) & (self.method == 'pearson'):
            for _ in range(self.iters):
                x_perm = np.random.permutation(self.x)
                r = abs(stats.pearsonr(x_perm , self.y)[0])
                corrs.append(r)
                
        elif (self.onesided == False) & (self.method == 'spearman'):
            for _ in range(self.iters):
                x_perm = np.random.permutation(self.x)
                r = abs(stats.spearmanr(self.x , self.y)[0])
                corrs.append(r)
        
        elif (self.onesided == True) & (self.method == 'pearson'):
            for _ in range(self.iters):
                x_perm = np.random.permutation(self.x)
                r = stats.pearsonr(x_perm , self.y)[0]
                corrs.append(r)
                
        elif (self.onesided == True) & (self.method == 'spearman'):
            for _ in range(self.iters):
                x_perm = np.random.permutation(self.x)
                r = stats.spearmanr(self.x , self.y)[0]
                corrs.append(r)
    
        else:
            raise ValueError('\'onesided\' parameter only accepts Boolean True or False, ' +
                             'and \'method\' only accepts \'pearson\' or \'spearman\'')   
        
        return np.array(corrs), DiscreteRv(corrs)
        
    def Power(self, alpha=0.05, num_runs=1000):
        """Computes the power of the hypothesis test. 

        Args
        ----
        alpha (float):
            The significance level for the hypothesis test.
            Must be between 0 and 1. Defaults to 0.05
        num_runs (int):
            The number of times to run the hypothesis test to compute power.
            Defaults to 1000.

        Returns
        -------
        power:
            Computed as the percentage of significant pvalues in num_runs of the test.
            Returned value is between 0 and 1.
        """      
        # Build the df to use to make the run data
        df = pd.DataFrame({'x':self.x, 'y': self.y})
        
        pvalue_count = 0
        
        for _ in range(num_runs):
            # Create run data
            run_df = df.sample(n=len(df), replace=True)
            run_x = run_df.x.values
            run_y = run_df.y.values
            run_data = run_x, run_y
        
            # Run the hypothesis test with run_data
            test = HTCorrelationH0(run_data, onesided=self.onesided, tail=self.tail, 
                                   iters=100, method=self.method)
            pvalue = test.PValue()
            
            if pvalue < alpha:
                pvalue_count += 1
            
        return pvalue_count / num_runs


class HTCorrelationHa(HypothesisTest):
    """This class is used to conduct correlation hypothesis testing. 
    Uses resampling of x, y pairs to build a sampling distribution of correlation statistics
    that represents the alternative hypothesis of correlation existing between the variables.
    A test_stat to represent the null hypothesis must be provided. 
    This test can only produce a onesided pvalue.
    Accepts data as a list or tuple of two groups of data (eg. (x, y))

    Parameters
    ----------
    data (array-like):
        A list or tuple of two groups of data (eg. (x, y))
    test_stat (float):
        The test stat to represent the null hypothesis
    tail (str):
        The tail of the distribution to be used in the PValue function
        Accepts only 'right' or 'left'
        Defaults to 'right'
    iters (int):
        The number of iterations to run in the ComputeRv function 
        Defaults to 1000
    method (str):
        The method to use for correlation calcluation.
        Accepts only 'pearson' or 'spearman'
        Defaults to 'pearson
    
    Attributes
    ----------
    data:
        The original data
    test_stat:
        The test statistic used in the hypothesis test
    sampling_dist:
        The sampling distribution generated by resampling
    rv:
        A scipy.stats discrete_rv object (random variable) 
        that represents the sampling distribution
        This object provides numerous useful attributes and methods
        See the discrete_rv documentation for details

    Methods
    -------
    PValue():
        Computes the p-value for the hypothesis test
    Power(alpha=0.05, num_runs=1000):
        Computes the power of the hypothesis test
        alpha: the significance level for the hypothesis test, default=0.05
        num_runs: the number of hypothesis tests to run, default=1000
    MinMaxTestStat():
        Returns the smallest and largest test statistics in the sampling distribution
    PlotCdf():
        Draws a Cdf of the distribution with a vertical line at the test stat
    """
    def __init__(self, data, test_stat, tail='right', iters=1000, method='pearson'):
        self.test_stat = test_stat
        self.method = method
        HypothesisTest.__init__(self, data, tail, iters)

    def PrepareData(self, data):
        x, y = data
        self.x = np.array(x)
        self.y = np.array(y)
        self.df = pd.DataFrame({'x':self.x, 'y': self.y})
              
    def ComputeRv(self):
        # Build the sampling distribution
        corrs = []
                
        if self.method == 'pearson':
            for _ in range(self.iters):
                sample = self.df.sample(n=len(self.df), replace=True)
                r = stats.pearsonr(sample.x, sample.y)[0]
                corrs.append(r)
                
        elif self.method == 'spearman':
            for _ in range(self.iters):
                sample = self.df.sample(n=len(self.df), replace=True)
                r = stats.spearmanr(sample.x, sample.y)[0]
                corrs.append(r)
    
        else:
            raise Exception('Must enter either \'pearson\' or \'spearman\' ' +
                            'as a string for the \'method\' argument')   
        
        return np.array(corrs), DiscreteRv(corrs)
    
    def Power(self, alpha=0.05, num_runs=1000):
        """Computes the power of the hypothesis test. 

        Args
        ----
        alpha (float):
            The significance level for the hypothesis test.
            Must be between 0 and 1. Defaults to 0.05
        num_runs (int):
            The number of times to run the hypothesis test to compute power.
            Defaults to 1000.

        Returns
        -------
        power:
            Computed as the percentage of significant pvalues in num_runs of the test.
            Returned value is between 0 and 1.
        """    
        pvalue_count = 0
        
        for _ in range(num_runs):
            # Create run data
            run_df = self.df.sample(n=len(self.df), replace=True)
            run_x = run_df.x.values
            run_y = run_df.y.values
            run_data = run_x, run_y
        
            # Run the hypothesis test with run_data
            test = HTCorrelationHa(run_data, test_stat=self.test_stat, tail=self.tail, 
                                   iters=100, method=self.method)
            pvalue = test.PValue()
            
            if pvalue < alpha:
                pvalue_count += 1
            
        return pvalue_count / num_runs


class HTChiSquare(HypothesisTest):
    """This class is used to conduct chi square hypothesis testing. 
    Uses resampling of the expected sequence to simulate the null hypothesis 
    and build the null hypothesis chi square statistic sampling distribution. 
    Accepts data in the form of a list or tuple of two sequences (observed, expected).
    The passed sequences must be the same length and the sum of the sequence values must be the same. 
    If the sum of the sequence values is different, first normalize the expected values 
    and then create a new expected values sequence by multiplying by the total number of observed values. 
    adjust_expected = expected/sum(expected)*sum(observed)

    Parameters
    ----------
    data (array-like):
        A list or tuple of two sequences (observed, expected)
    tail (str):
        The tail of the distribution to be used in the PValue function
        Accepts only 'right' or 'left'
        Defaults to 'right'
    iters (int):
        The number of iterations to run in the ComputeRv function 
        Defaults to 1000
    
    Attributes
    ----------
    data:
        The original data
    test_stat:
        The test statistic used in the hypothesis test
    sampling_dist:
        The sampling distribution generated by resampling
    rv:
        A scipy.stats discrete_rv object (random variable) 
        that represents the sampling distribution
        This object provides numerous useful attributes and methods
        See the discrete_rv documentation for details

    Methods
    -------
    PValue():
        Computes the p-value for the hypothesis test
    Power(alpha=0.05, num_runs=1000):
        Computes the power of the hypothesis test
        alpha: the significance level for the hypothesis test, default=0.05
        num_runs: the number of hypothesis tests to run, default=1000
    MinMaxTestStat():
        Returns the smallest and largest test statistics in the sampling distribution
    PlotCdf():
        Draws a Cdf of the distribution with a vertical line at the test stat
    """
    def PrepareData(self, data):
        self.observed, self.expected = data
        self.observed = np.array(self.observed)
        self.expected = np.array(self.expected)
        
        # Check that sum of values are equal
        if np.isclose(sum(self.observed), sum(self.expected)) == False:
            raise ValueError('The sum of the values for observed and expected must be equal.')
        
    def TestStat(self):
        self.test_stat = sum((self.observed - self.expected)**2 / self.expected)
        
    def ComputeRv(self):
        # Calculate the variables needed for resampling        
        n = sum(self.expected)
        values = list(range(len(self.expected)))
        p_exp = self.expected/sum(self.expected)
        
        # Build the sampling distribution
        chis = []
        
        for _ in range(self.iters):          
            hist = Counter({x:0 for x in values}) # Initialize a Counter with zero values
            hist.update(np.random.choice(values, size=n, replace=True, p=p_exp))
            sorted_hist = sorted(hist.items())
            model_observed = np.array([x[1] for x in sorted_hist])
            chi = sum((model_observed - self.expected)**2 / self.expected)
                
            chis.append(chi)
            
        return np.array(chis), DiscreteRv(chis)
    
    def Power(self, alpha=0.05, num_runs=1000):
        """Computes the power of the hypothesis test. 

        Args
        ----
        alpha (float):
            The significance level for the hypothesis test.
            Must be between 0 and 1. Defaults to 0.05
        num_runs (int):
            The number of times to run the hypothesis test to compute power.
            Defaults to 1000.

        Returns
        -------
        power:
            Computed as the percentage of significant pvalues in num_runs of the test.
            Returned value is between 0 and 1.
        """    
        pvalue_count = 0
        
        for _ in range(num_runs):
            # Create a new run_observed by resampling the observed sequence 
            # Then create the new run_data using run_observed and the original expected sequence
            n = sum(self.observed)
            values_obs = list(range(len(self.observed)))
            p_obs = self.observed/sum(self.observed)
        
            hist = Counter({x:0 for x in values_obs})
            hist.update(np.random.choice(values_obs, size=n, replace=True, p=p_obs))
            sorted_hist = sorted(hist.items())
            run_observed = np.array([x[1] for x in sorted_hist])
            run_data = run_observed, self.expected

            # Run the hypothesis test with run_data
            test = HTChiSquare(run_data, tail=self.tail, iters=100)
            pvalue = test.PValue()
            
            if pvalue < alpha:
                pvalue_count += 1
            
        return pvalue_count / num_runs


class HTChiSquareContingency(HypothesisTest):
    """This class is used to conduct chi square contingency table hypothesis testing. 
    Uses resampling of the expected sequence to simulate the null hypothesis 
    and build the null hypothesis sampling distribution. 
    Accepts data in the form of a single observed contingency table (array-like).
    
    Parameters
    ----------
    data (array-like):
        Must be in the form of a single observed contingency table
    tail (str):
        The tail of the distribution to be used in the PValue function
        Accepts only 'right' or 'left'
        Defaults to 'right'
    iters (int):
        The number of iterations to run in the ComputeRv function 
        Defaults to 1000
    
    Attributes
    ----------
    data:
        The original data
    test_stat:
        The test statistic used in the hypothesis test
    sampling_dist:
        The sampling distribution generated by resampling
    rv:
        A scipy.stats discrete_rv object (random variable) 
        that represents the sampling distribution
        This object provides numerous useful attributes and methods
        See the discrete_rv documentation for details

    Methods
    -------
    PValue():
        Computes the p-value for the hypothesis test
    Power(alpha=0.05, num_runs=1000):
        Computes the power of the hypothesis test
        alpha: the significance level for the hypothesis test, default=0.05
        num_runs: the number of hypothesis tests to run, default=1000
    MinMaxTestStat():
        Returns the smallest and largest test statistics in the sampling distribution
    PlotCdf():
        Draws a Cdf of the distribution with a vertical line at the test stat
    """
    def PrepareData(self, data):
        self.observed = data
        self.observed = np.array(self.observed)
        
    # In the case of this class, TestStat also computes self.expected 
    # for later use in the ComputeRv function
    def TestStat(self):
        self.test_stat,_,_,self.expected = stats.chi2_contingency(self.observed)
        
    def ComputeRv(self):
        # Calculate the variables needed for resampling        
        expected_shape = self.expected.shape
        expected_ps = self.expected / np.sum(self.expected)
        values = np.array(list(range(len(self.expected.ravel())))) # Flatten the array
        n= int(np.sum(self.expected))
        
        # Build the sampling distribution
        chis = []
        
        for _ in range(self.iters):          
            # Initiate an empty histogram to hold resampled values
            hist = Counter({x:0 for x in values})
            
            hist.update(np.random.choice(values, size=n, replace=True, p=expected_ps.ravel()))
            sorted_hist = sorted(hist.items())
            resampled_expected = np.array([x[1] for x in sorted_hist])
            
            # Put the array back into its original shape
            resampled_expected_reshaped = resampled_expected.reshape(expected_shape)

            chi = stats.chi2_contingency(resampled_expected_reshaped)[0]                
            chis.append(chi)
            
        return np.array(chis), DiscreteRv(chis)
    
    def Power(self, alpha=0.05, num_runs=1000):
        """Computes the power of the hypothesis test. 

        Args
        ----
        alpha (float):
            The significance level for the hypothesis test.
            Must be between 0 and 1. Defaults to 0.05
        num_runs (int):
            The number of times to run the hypothesis test to compute power.
            Defaults to 1000.

        Returns
        -------
        power:
            Computed as the percentage of significant pvalues in num_runs of the test.
            Returned value is between 0 and 1.
        """
        pvalue_count = 0
        
        for _ in range(num_runs):
            # Create run_data by resampling the observed data    
            observed_shape = self.observed.shape
            observed_ps = self.observed / np.sum(self.observed)
            values = np.array(list(range(len(self.observed.ravel())))) # Flatten the array
            n= int(np.sum(self.observed))
        
            # Initiate an empty histogram to hold resampled values
            hist = Counter({x:0 for x in values})
            
            hist.update(np.random.choice(values, size=n, replace=True, p=observed_ps.ravel()))
            sorted_hist = sorted(hist.items())
            resampled_observed = np.array([x[1] for x in sorted_hist])
            run_data = resampled_observed.reshape(observed_shape) # Put back into original shape

            # Run the hypothesis test with run_data
            test = HTChiSquareContingency(run_data, tail=self.tail, iters=100)
            pvalue = test.PValue()
            
            if pvalue < alpha:
                pvalue_count += 1
            
        return pvalue_count / num_runs


class HTOnewayAnova(HypothesisTest):
    """This class is used to conduct one-way ANOVA hypothesis tesing. 
    Uses permutation of the supplied data sequences to build the null hypothesis 
    and f-statistic sampling distribution. 
    Accepts data in the form of a list of data sequences (group_1, group_2... group_n).

    Parameters
    ----------
    data (array-like):
        A list or tuple of data sequences (group_1, group_2... group_n)
    tail (str):
        The tail of the distribution to be used in the PValue function
        Accepts only 'right' or 'left'
        Defaults to 'right'
    iters (int):
        The number of iterations to run in the ComputeRv function 
        Defaults to 1000
    
    Attributes
    ----------
    data:
        The original data
    test_stat:
        The test statistic used in the hypothesis test
    sampling_dist:
        The sampling distribution generated by resampling
    rv:
        A scipy.stats discrete_rv object (random variable) 
        that represents the sampling distribution
        This object provides numerous useful attributes and methods
        See the discrete_rv documentation for details

    Methods
    -------
    PValue():
        Computes the p-value for the hypothesis test
    Power(alpha=0.05, num_runs=1000):
        Computes the power of the hypothesis test
        alpha: the significance level for the hypothesis test, default=0.05
        num_runs: the number of hypothesis tests to run, default=1000
    MinMaxTestStat():
        Returns the smallest and largest test statistics in the sampling distribution
    PlotCdf():
        Draws a Cdf of the distribution with a vertical line at the test stat
    """
    def PrepareData(self, data):
        self.pooled_data = np.hstack(data)
        
    def TestStat(self):
        self.test_stat, _ = stats.f_oneway(*self.data)
        
    def ComputeRv(self):    
        # Initialize the sampling distribution list 
        # that will hold the f-stats calculated for each iteration 
        f_stats = []
        
        for _ in range(self.iters):
            # Permute the pooled data
            pooled_data_perm = np.random.permutation(self.pooled_data)
            
            # For each item in the orignal data 
            # pull out the correct number of permuted values 
            # and store the new sequences in a list
            data_perm_list = []
            for x in self.data:
                x_perm = pooled_data_perm[:len(x)]
                data_perm_list.append(x_perm)
    
                pooled_data_perm = pooled_data_perm[len(x):]
                
            # Compute the f-stat for the permuted data 
            # and add it to the sampling distribution list
            f_stat, _ = stats.f_oneway(*data_perm_list)
            f_stats.append(f_stat)
            
        return np.array(f_stats), DiscreteRv(f_stats)
    
    def Power(self, alpha=0.05, num_runs=1000):
        """Computes the power of the hypothesis test. 

        Args
        ----
        alpha (float):
            The significance level for the hypothesis test.
            Must be between 0 and 1. Defaults to 0.05
        num_runs (int):
            The number of times to run the hypothesis test to compute power.
            Defaults to 1000.

        Returns
        -------
        power:
            Computed as the percentage of significant pvalues in num_runs of the test.
            Returned value is between 0 and 1.
        """    
        pvalue_count = 0
        
        for _ in range(num_runs):
            # Simulate resampling from the population
            run_data_resample_list = []
            for x in self.data:
                x_run_resample = np.random.choice(x, size=len(x), replace=True)
                run_data_resample_list.append(x_run_resample)
            
            # Run the hypothesis test with run_data
            test = HTOnewayAnova(run_data_resample_list, tail=self.tail, iters=100)
            pvalue = test.PValue()
            
            if pvalue < alpha:
                pvalue_count += 1
            
        return pvalue_count / num_runs


def ExpectedFromObserved(obs):
    """Adapted from scipy stats chi2_contingency:
    https://github.com/scipy/scipy/blob/v1.8.1/scipy/stats/contingency.py#L132-L301  
    Takes an observed frequency array (contingency table) and computes the expected frequency array.

    Args:
        obs (array-like): The observed frequency array
  
    Returns:
        array: The expected frequency array
    """
    obs_array = np.asarray(obs, dtype=np.float64)
    
    margsums = []
    ranged = list(range(obs_array.ndim))
    for k in ranged:
        marg = np.apply_over_axes(np.sum, obs_array, [j for j in ranged if j != k])
        margsums.append(marg)

    d = obs_array.ndim
    expected = reduce(np.multiply, margsums) / obs_array.sum() ** (d - 1)
    
    return expected


def ChiSquareContribution(obs, exp):
    """Calculates the Chi square contribution for each element in contingency table (observed array). 
    Can use the ExpectedFromObserved function in this module 
    to obtain an expected array from a contingency table (observed array). 
    The shape of the observed and expected arrays must be the same. 
    If a DataFrame is used for the contingency table (observed array), 
    the index and column names are maintained and a DataFrame is returned, 
    otherwise an array is returned.

    Args:
        obs (array-like): The observed frequency array
        exp (array-like): The expected frequency array

    Returns:
        array: Chi square contribution array
    """
    obs_array = np.array(obs)
    exp_array = np.array(exp)
    
    if obs_array.shape != exp_array.shape:
        raise Exception ("The shape of the observed and expected arrays must be the same.")
    
    result_array = (obs_array - exp_array)**2/exp_array
    
    if isinstance(obs, pd.DataFrame):
        ix = obs.index
        cols = obs.columns
        
        return pd.DataFrame(data=result_array, index=ix, columns=cols)
    
    else:
        return result_array


def AnovaPostHoc(data, labels=None, alpha=0.05):
    """Performs ANOVA post-hoc analysis using a difference of means hypothesis test 
    on each possible pairing of supplied data sequences.
    This analysis is used to determine which pairs 
    have statistically significant differences in their means. 
    If starting from a DataFrame that has the data groups 
    and their corresponding values in long format, 
    the DataFrameToList function in the DataStats singlevar module 
    can be used to get the data into the format needed for use in this function. 
    This is the equivalent of using the pairwise_tukeyhsd function from statsmodels.stats.multicomp.
        
    Args
    ----
    data (array-like):
        A list or tuple of data sequences (group_1, group_2... group_n)
    labels (array-like):
        A list or tuple containing the labels for each group in the data. 
        The number of labels must equal the number of groups 
        and must be in the correct order to match up with each group. 
        If no labels are given then numerical labels are used by default.
    alpha (float)
        The family wise error rate (FWER)
        Must be between 0 and 1. Defaults to 0.05.
    
    Returns
    -------
    results:
        A tuple of tuples containing the results for each pairing of sequences. 
        The results can be printed in an easy-to-read format using a for loop. 
        The results display three pieces of information:
        1) The pairing, uses labels if provided
        2) The pvalue for the pairing
        3) Whether the pvalue is significant or not (Y or N)
           * The significance level is determined by comparison of the pvalue 
             with the experiment-wise significance level. 
             The Bonferroni correction method is used to compute this significance level.
    corrected_alpha:
        The experiment-wise significance level
    """
    # Calculate the experiment-wise significance level
    num_comparisons = int(factorial(len(data))/(2*factorial(len(data)-2)))
    corrected_alpha = alpha/num_comparisons

    # Instantiate a results list
    results=[]
    
    # Compute the results in the case that labels have been provided
    if labels is not None:
        if len(data) != len(labels):
            raise Exception ("The number of labels must equal the number of data groups.")
            
        zip_data = zip(labels, data)
        
        for pair in combinations(zip_data, 2):
            test = HTDiffMeansH0((pair[0][1], pair[1][1]))
            pvalue = test.PValue()
            significant = 'Y' if pvalue < corrected_alpha else 'N'
            results.append(((pair[0][0], pair[1][0]), '{:.4g}'.format(pvalue), significant))
        
    # Compute the results in the case that labels have not been provided
    else:
        
        enum_data = enumerate(data)
        
        for pair in combinations(enum_data, 2):
            test = HTDiffMeansH0((pair[0][1], pair[1][1]))
            pvalue = test.PValue()
            significant = 'Y' if pvalue < corrected_alpha else 'N'
            results.append(((pair[0][0], pair[1][0]), '{:.4g}'.format(pvalue), significant))
    
    return results, corrected_alpha


def main():
    pass

if __name__ == '__main__':
    main()