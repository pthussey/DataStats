{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime as dt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.formula.api as smf\n",
    "import patsy\n",
    "\n",
    "import dataStatsAnalysis_old as dsa\n",
    "import dataStatsPlotting as dsp\n",
    "\n",
    "dsp.SetParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PowerMean(data, test_mean, alpha=0.05, alternative='two-sided', num_runs=100):\n",
    "    a = np.array(data)\n",
    "    \n",
    "    # Set counter for significant p-values to zero\n",
    "    count = 0\n",
    "        \n",
    "    # Run resampling of the array to simulate the alternative hypothesis of an existing difference from the test mean\n",
    "    for i in range(num_runs):\n",
    "        run_data = np.random.choice(a, size=len(a), replace=True)\n",
    "            \n",
    "        # Build a sampling distribution for the run\n",
    "        mean_estimates = [np.random.choice(run_data, size=len(run_data), replace=True).mean() for _ in range(100)]\n",
    "            \n",
    "        # Create an rv of the results and calculate left and right side p-values\n",
    "        rv = dsa.DiscreteRv(mean_estimates)\n",
    "        p_value_right = 1 - rv.cdf(test_mean)\n",
    "        p_value_left = rv.cdf(test_mean)\n",
    "        \n",
    "        # Case of a two-sided test\n",
    "        if alternative == 'two-sided':\n",
    "            if (p_value_right < alpha/2) or (p_value_left < alpha/2):\n",
    "                count += 1\n",
    "        \n",
    "        # Case of testing for an effect that is smaller than the test mean\n",
    "        elif alternative == 'smaller': \n",
    "            if p_value_right < alpha:\n",
    "                count += 1\n",
    "        \n",
    "        # Case of testing for an effect that is larger than the test mean\n",
    "        elif alternative == 'larger': \n",
    "            if p_value_left < alpha:\n",
    "                count += 1\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"alternative has to be 'two-sided', 'smaller', or 'larger'\")\n",
    "            \n",
    "    return count / num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnimplementedMethodException(Exception):\n",
    "    \"\"\"Exception if someone calls a method that should be overridden.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PowerTest superclass\n",
    "class PowerTest():\n",
    "    \n",
    "    def __init__(self, data, test_stat, alpha=0.05, alternative='two-sided', num_runs=100):\n",
    "        self.data = data\n",
    "        self.test_stat = test_stat\n",
    "        self.alpha = alpha\n",
    "        self.alternative = alternative\n",
    "        self.num_runs = num_runs\n",
    "        self.PrepareData()\n",
    "    \n",
    "    # Provide functionality to convert the data into format needed for use in BuildRv\n",
    "    # Ex. Convert to array, split data into component groups, etc.\n",
    "    # See child classes for examples\n",
    "    def PrepareData(self):\n",
    "        UnimplementedMethodException()\n",
    "    \n",
    "    # Provide functionality to create a sampling distribution and build an rv\n",
    "    # This involves doing one resample to simulate pulling an additional sample from the population,\n",
    "    # then building building a sampling distribution for that sample,\n",
    "    # and finally building an rv from the sampling distribution\n",
    "    # See child classes for examples\n",
    "    def BuildRv(self):\n",
    "        UnimplementedMethodException()\n",
    "    \n",
    "    # Computes the pvalue of test stat from an rv,\n",
    "    # and adds to pvalue_count if less than significance level\n",
    "    def _RunPvalueCount(self):\n",
    "        rv = self.BuildRv()\n",
    "        p_value_right = 1 - rv.cdf(self.test_stat)\n",
    "        p_value_left = rv.cdf(self.test_stat)\n",
    "        \n",
    "        # Case of a two-sided test\n",
    "        if self.alternative == 'two-sided':\n",
    "            if (p_value_right < self.alpha/2) or (p_value_left < self.alpha/2):\n",
    "                self.pvalue_count+= 1\n",
    "        \n",
    "        # Case of testing for an effect that is smaller than the test mean\n",
    "        elif self.alternative == 'smaller': \n",
    "            if p_value_right < self.alpha:\n",
    "                self.pvalue_count += 1\n",
    "        \n",
    "        # Case of testing for an effect that is larger than the test mean\n",
    "        elif self.alternative == 'larger': \n",
    "            if p_value_left < self.alpha:\n",
    "                self.pvalue_count += 1\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"alternative has to be 'two-sided', 'smaller', or 'larger'\")\n",
    "    \n",
    "    # Method for computing power \n",
    "    def Power(self):\n",
    "        self.pvalue_count = 0\n",
    "        for i in range(self.num_runs):\n",
    "            self._RunPvalueCount()\n",
    "            \n",
    "        return self.pvalue_count / self.num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTMean(PowerTest):\n",
    "    \n",
    "    def PrepareData(self):\n",
    "        self.a = np.array(self.data)\n",
    "    \n",
    "    def BuildRv(self):\n",
    "        run_data = np.random.choice(self.a, size=len(self.a), replace=True)\n",
    "        mean_estimates = [np.random.choice(run_data, size=len(run_data), replace=True).mean() for _ in range(100)]\n",
    "        rv = dsa.DiscreteRv(mean_estimates)\n",
    "        return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_data = np.random.randint(-8,11,size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "powmean = PTMean(mean_data, 0, alternative='two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "powmean.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PowerMean(mean_data, 0, alternative='two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PowerDiffMeans(a, b, test_diff_means, alpha = 0.05, alternative = 'two-sided', num_runs=100):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    \n",
    "    # Set counter for significant p-values to zero\n",
    "    count = 0\n",
    "    \n",
    "    # Run resampling of the arrays separately (not pooled) to simulate the alternative hypothesis of a difference existing\n",
    "    for i in range(num_runs):\n",
    "        sample1 = np.random.choice(a, size=len(a), replace=True)\n",
    "        sample2 = np.random.choice(b, size=len(b), replace=True)\n",
    "        \n",
    "        diff_mean_results = []\n",
    "        \n",
    "        # Build a sampling distribution for the run\n",
    "        for j in range(100):\n",
    "            group1 = np.random.choice(sample1, size=len(sample1), replace=True)\n",
    "            group2 = np.random.choice(sample2, size=len(sample2), replace=True)\n",
    "            result = group1.mean() - group2.mean()\n",
    "            diff_mean_results.append(result)\n",
    "        \n",
    "        # Create an rv of the results and calculate left and right side p-values\n",
    "        rv = dsa.DiscreteRv(diff_mean_results)\n",
    "        p_value_right = 1 - rv.cdf(test_diff_means)\n",
    "        p_value_left = rv.cdf(test_diff_means)        \n",
    "\n",
    "        # Case of a two-sided test\n",
    "        if alternative == 'two-sided':\n",
    "            if (p_value_right < alpha/2) or (p_value_left < alpha/2):\n",
    "                count += 1\n",
    "        \n",
    "        # Case of testing for an effect that is smaller than the test difference of means\n",
    "        elif alternative == 'smaller': \n",
    "            if p_value_right < alpha:\n",
    "                count += 1\n",
    "        \n",
    "        # Case of testing for an effect that is larger than the test difference of means\n",
    "        elif alternative == 'larger': \n",
    "            if p_value_left < alpha:\n",
    "                count += 1\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"alternative has to be 'two-sided', 'smaller', or 'larger'\")\n",
    "    \n",
    "    return count / num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTDiffMeans(PowerTest):\n",
    "    \n",
    "    def PrepareData(self):\n",
    "        self.a, self.b = self.data\n",
    "        self.a = np.array(self.a)\n",
    "        self.b = np.array(self.b)\n",
    "    \n",
    "    def BuildRv(self):\n",
    "        # Create run data\n",
    "        sample1 = np.random.choice(self.a, size=len(self.a), replace=True)\n",
    "        sample2 = np.random.choice(self.b, size=len(self.b), replace=True)\n",
    "        \n",
    "        diff_mean_results = []\n",
    "        \n",
    "        # Build a sampling distribution for the run\n",
    "        for j in range(100):\n",
    "            group1 = np.random.choice(sample1, size=len(sample1), replace=True)\n",
    "            group2 = np.random.choice(sample2, size=len(sample2), replace=True)\n",
    "            result = group1.mean() - group2.mean()\n",
    "            diff_mean_results.append(result)\n",
    "        \n",
    "        rv = dsa.DiscreteRv(diff_mean_results)\n",
    "        \n",
    "        return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import first\n",
    "\n",
    "live, firsts, others = first.MakeFrames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [firsts.prglngth.values, others.prglngth.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffmeans = PTDiffMeans(data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seems to be working\n",
    "diffmeans.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PowerDiffMeans(firsts.prglngth.values, others.prglngth.values, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4413"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(firsts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38.60095173351461, 38.52291446673706, 2.791585069824391, 2.6155761106844744)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gpower gives a power of 0.41, very close\n",
    "np.mean(firsts.prglngth.values), np.mean(others.prglngth.values), np.std(firsts.prglngth.values), np.std(others.prglngth.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTCorrelation(PowerTest):\n",
    "    def __init__(self, data, test_stat, alpha=0.05, alternative='two-sided', num_runs=100, method='pearson'):\n",
    "        PowerTest.__init__(self, data, test_stat, alpha, alternative, num_runs)\n",
    "        self.method = method\n",
    "    \n",
    "    def PrepareData(self):\n",
    "        self.x, self.y = self.data\n",
    "        self.df = pd.DataFrame({'x':self.x, 'y': self.y})\n",
    "    \n",
    "    def BuildRv(self):\n",
    "        # Create run data\n",
    "        run_data = self.df.sample(n=len(self.df), replace=True)\n",
    "        \n",
    "        corrs=[]\n",
    "        \n",
    "        # Build rv\n",
    "        if self.method == 'pearson':          \n",
    "            for _ in range(100):\n",
    "                sample = run_data.sample(n=len(run_data), replace=True)\n",
    "                r = stats.pearsonr(sample.x, sample.y)[0]\n",
    "                corrs.append(r)\n",
    "    \n",
    "        elif self.method == 'spearman':  \n",
    "            for _ in range(100):\n",
    "                sample = run_data.sample(n=len(run_data), replace=True)\n",
    "                r = stats.spearmanr(sample.x, sample.y)[0]\n",
    "                corrs.append(r)\n",
    "    \n",
    "        else:\n",
    "            raise Exception('Must enter either pearson or spearman as a string for method argument')\n",
    "               \n",
    "        rv = dsa.DiscreteRv(corrs)\n",
    "        \n",
    "        return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = live.dropna(subset=['agepreg', 'totalwgt_lb'])\n",
    "data = cleaned.agepreg.values, cleaned.totalwgt_lb.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "powcorr = PTCorrelation(data,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "powcorr.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = sns.load_dataset('penguins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>MALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
       "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
       "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
       "3  Adelie  Torgersen             NaN            NaN                NaN   \n",
       "4  Adelie  Torgersen            36.7           19.3              193.0   \n",
       "\n",
       "   body_mass_g     sex  \n",
       "0       3750.0    MALE  \n",
       "1       3800.0  FEMALE  \n",
       "2       3250.0  FEMALE  \n",
       "3          NaN     NaN  \n",
       "4       3450.0  FEMALE  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = penguins.bill_length_mm.dropna()\n",
    "depth = penguins.bill_depth_mm.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.23505287035553274, 1.119662196137215e-05)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.pearsonr(length, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = length.values, depth.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "powcorr_penguins = PTCorrelation(data,0)\n",
    "powcorr_penguins.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(342, 342)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(penguins.bill_length_mm.dropna()), len(penguins.bill_depth_mm.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat      cut color clarity  depth  table  price     x     y     z\n",
       "0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
       "1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
       "2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
       "3   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
       "4   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = sns.load_dataset('diamonds')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.01064740458414299, 0.013403249011863443)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.pearsonr(df_test.depth, df_test.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_test.depth.values, df_test.price.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Might be working, not sure how to confirm\n",
    "# Comparison with Gpower results (z-tests - correlations: two independent Pearson's rs) is not close (0.41)\n",
    "# Also strange that changing to smaller alternative didn't change the result of the calculation\n",
    "testpowcorr = PTCorrelation(data,0, alternative='smaller')\n",
    "testpowcorr.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53940"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test.price.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  8, 11,  3,  3, 14, 10,  5, 13,  1, 19,  7, 11,  7,  7,  8,  8,\n",
       "       10,  9,  4, 17, 18,  5, 11,  5,  9,  2, 15, 11, 19,  3,  7,  1,  5,\n",
       "        6,  4,  6,  8,  2, 15, 17, 19,  5, 13, 14,  4, 19, 15,  9, 16])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find a smaller correlation data set to test this with\n",
    "a = np.random.randint(1,20,50)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17, 10,  8, 16,  6,  4,  1, 13,  7, 16, 16, 18, 16, 13, 14, 11,  9,\n",
       "       12,  1, 19, 11,  8,  3,  5, 15,  7,  4, 15,  4,  8, 16, 14, 12, 15,\n",
       "       12, 19, 13, 13, 16,  4,  4, 19,  1, 17, 17, 19,  7, 14,  6,  6])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.random.randint(1,20,50)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.19846434180924646, 0.1670798494325032)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.pearsonr(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = a,b\n",
    "testpowcorr2 = PTCorrelation(data2, 0, alternative='two-sided')\n",
    "testpowcorr2.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A test with same data from Hypothesis test... notebook\n",
    "c = [12,  7,  2, 12, 11,  5, 15,  5,  6, 12,  5, 15, 10,  2,  8,  2,  5,\n",
    "       16, 16,  2, 17,  7, 11, 13, 13, 13, 17,  4,  9, 15, 13,  5,  5, 19,\n",
    "        3,  4, 11, 16,  4, 14,  7, 12, 14,  9,  8,  4,  2,  8,  8,  8]\n",
    "c = np.array(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [13,  3,  8, 18, 12,  8, 14, 19,  5,  2, 10, 17,  6, 12,  3,  2, 18,\n",
    "        6, 11,  7, 12, 18, 16, 13, 13, 14, 19,  2, 14, 17,  3,  5, 14,  2,\n",
    "       10, 14,  9, 11,  2,  5, 11, 18,  3,  4, 18,  1, 11,  4, 18,  3]\n",
    "d= np.array(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Yields same result so this doesn't appear to be a problem with using a class instead of a function\n",
    "data3 = c,d\n",
    "testpowcorr3 = PTCorrelation(data3, 0, alternative='two-sided')\n",
    "testpowcorr3.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a test of the two correlation hypothesis test methods (H0 and Ha) it seems there is quite a difference\n",
    "# See test near bottom of Hypothesis test... notebook\n",
    "# Try building a power test with the null hypothesis method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTCorrelationH0(PowerTest):\n",
    "    def __init__(self, data, test_stat, alpha=0.05, alternative='two-sided', num_runs=100, method='pearson'):\n",
    "        PowerTest.__init__(self, data, test_stat, alpha, alternative, num_runs)\n",
    "        self.method = method\n",
    "    \n",
    "    def PrepareData(self):\n",
    "        self.x, self.y = self.data\n",
    "    \n",
    "    def BuildRv(self):\n",
    "        # Create run data\n",
    "        run_x = np.random.permutation(self.x)\n",
    "        run_y = self.y\n",
    "        \n",
    "        corrs=[]\n",
    "        \n",
    "        # Build rv\n",
    "        if self.method == 'pearson':          \n",
    "            for _ in range(100):\n",
    "                x_perm = np.random.permutation(run_x)\n",
    "                r = stats.pearsonr(x_perm , run_y)[0]\n",
    "                corrs.append(r)\n",
    "    \n",
    "        elif self.method == 'spearman':  \n",
    "            for _ in range(100):\n",
    "                x_perm = np.random.permutation(run_x)\n",
    "                r = stats.pearsonr(x_perm , run_y)[0]\n",
    "                corrs.append(r)\n",
    "    \n",
    "        else:\n",
    "            raise Exception('Must enter either pearson or spearman as a string for method argument')   \n",
    "        \n",
    "        rv = dsa.DiscreteRv(corrs)\n",
    "        \n",
    "        return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1684736086385471, 0.24218627009242552)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need the correlation from the orginal\n",
    "# Interesting, the p-value from this is close to the one from the Ha computation not the H0 one\n",
    "corr_original = stats.pearsonr(c,d)\n",
    "corr_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is it working? gives almost zero power, sometimes 0.01\n",
    "# Gpower gives 0.13\n",
    "corr0 = PTCorrelationH0(data3, corr_original[0])\n",
    "corr0.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test original correlation hypothesis tests to see if there is a significant difference in pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "      <th>abbrev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>18.8</td>\n",
       "      <td>7.332</td>\n",
       "      <td>5.640</td>\n",
       "      <td>18.048</td>\n",
       "      <td>15.040</td>\n",
       "      <td>784.55</td>\n",
       "      <td>145.08</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18.1</td>\n",
       "      <td>7.421</td>\n",
       "      <td>4.525</td>\n",
       "      <td>16.290</td>\n",
       "      <td>17.014</td>\n",
       "      <td>1053.48</td>\n",
       "      <td>133.93</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>18.6</td>\n",
       "      <td>6.510</td>\n",
       "      <td>5.208</td>\n",
       "      <td>15.624</td>\n",
       "      <td>17.856</td>\n",
       "      <td>899.47</td>\n",
       "      <td>110.35</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total  speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "0   18.8     7.332    5.640          18.048       15.040       784.55   \n",
       "1   18.1     7.421    4.525          16.290       17.014      1053.48   \n",
       "2   18.6     6.510    5.208          15.624       17.856       899.47   \n",
       "\n",
       "   ins_losses abbrev  \n",
       "0      145.08     AL  \n",
       "1      133.93     AK  \n",
       "2      110.35     AZ  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car = sns.load_dataset('car_crashes')\n",
    "car.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.15689520004339752, 0.2715478689798989)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.pearsonr(car.no_previous, car.ins_premium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27999999999999947"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking pvalue from null hypothesis resampling\n",
    "# It's very close to that of stats one above\n",
    "resultH0 = dsa.ResampleCorrelationH0(car.no_previous.values, car.ins_premium.values)\n",
    "dsa.PValueFromEstimates(resultH0[1],resultH0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResampleCorrelation_Ha(x, y, iters=1000, method='pearson'):\n",
    "    \"\"\"Generates a correlation sampling distribution for the alternative hypothesis of correlation existing between the variables. \n",
    "    This is done by resampling x, y pairs and calculating correlation on new samples. \n",
    "    Can then make an rv of this distribution to calculate sampling distribution mean, std deviation (std error), and confidence interval (rv.interval). \n",
    "    Can also get a one-sided p-value for case of no difference null hypothesis using rv.cdf(0). \n",
    "    For two-sided p-value, can double the one-sided if sampling distribution is symmetrical or use the H0 version of this function. \n",
    "    Can also use the 'min' and 'max' built-ins to find what the most extreme values are from the simluations.\n",
    "\n",
    "    Args:\n",
    "        x (array-like): Input variable 1\n",
    "        y (array-like): Input variable 2\n",
    "        iters (int): The number of simulations to run (Defaults to 1000)\n",
    "        method (string): Select 'pearson' or 'spearman' method (default: 'pearson')\n",
    "        \n",
    "    Returns:\n",
    "        actual_r: Original actual correlation value\n",
    "        corrs (array): Sampling distribution for the alternative hypothesis of no correlation obtained from resampling\n",
    "    \"\"\"\n",
    "    if method == 'pearson':  \n",
    "        # Calculate actual correlation\n",
    "        actual_r = stats.pearsonr(x, y)[0]\n",
    "\n",
    "        # Create a dataframe to hold the x and y values as pairs\n",
    "        df = pd.DataFrame({'x':x, 'y': y})\n",
    "\n",
    "        corrs=[]    \n",
    "        for _ in range(iters):\n",
    "            sample = df.sample(n=len(df), replace=True)\n",
    "            r = stats.pearsonr(sample.x, sample.y)[0]\n",
    "            corrs.append(r)\n",
    "    \n",
    "    elif method == 'spearman':\n",
    "        # Calculate actual correlation\n",
    "        actual_r = stats.spearmanr(x, y)[0]\n",
    "\n",
    "        # Create a dataframe to hold the x and y values as pairs\n",
    "        df = pd.DataFrame({'x':x, 'y': y})\n",
    "\n",
    "        corrs=[]    \n",
    "        for _ in range(iters):\n",
    "            sample = df.sample(n=len(df), replace=True)\n",
    "            r = stats.spearmanr(sample.x, sample.y)[0]\n",
    "            corrs.append(r)\n",
    "    \n",
    "    else:\n",
    "        raise Exception('Must enter either pearson or spearman as a string for method argument')\n",
    "      \n",
    "    return actual_r, np.array(corrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18299999999999939"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking pvalue from alternative hypothesis resampling\n",
    "# It's lower but still close to the stats one above\n",
    "resultHa = ResampleCorrelation_Ha(car.no_previous, car.ins_premium)\n",
    "dsa.PValueFromEstimates(resultHa[1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = car.no_previous, car.ins_premium\n",
    "car_power = PTCorrelation(data,0)\n",
    "car_power.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = car.no_previous, car.ins_premium\n",
    "car_powerH0 = PTCorrelationH0(data,-0.1568)\n",
    "car_powerH0.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G-power gives 0.19 for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how my power diff means one compares with GPower result\n",
    "# Did this above and very similar result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
