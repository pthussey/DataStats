{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime as dt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.formula.api as smf\n",
    "import patsy\n",
    "\n",
    "import dataStatsAnalysis_old as dsa\n",
    "import dataStatsPlotting as dsp\n",
    "\n",
    "dsp.SetParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerTest():\n",
    "    \n",
    "    def __init__(self, data, alpha=0.05, alternative='two-sided', num_runs=100):\n",
    "        self.data = data\n",
    "        self.alpha = alpha\n",
    "        self.alternative = alternative\n",
    "        self.num_runs = num_runs\n",
    "        self.PrepareData()\n",
    "    \n",
    "    # Provide functionality to convert the data into format needed for use in BuildRv\n",
    "    # Ex. Convert to array, split data into component groups, etc.\n",
    "    # See child classes for examples\n",
    "    def PrepareData(self):\n",
    "        UnimplementedMethodException()\n",
    "    \n",
    "    # Provide functionality that creates the run data and then computes the run's test stat and rv\n",
    "    # This involves doing one resample to simulate pulling an additional sample from the population,\n",
    "    # then calculating the test_stat, building a sampling distribution, and computing the rv\n",
    "    # See child classes for examples\n",
    "    def ComputeRVandTestStat(self):\n",
    "        UnimplementedMethodException()\n",
    "    \n",
    "    # Computes the pvalue of test stat from an rv,\n",
    "    # and adds to pvalue_count if less than significance level\n",
    "    def _RunPvalueCount(self):\n",
    "        test_stat, rv = self.ComputeRVandTestStat()\n",
    "        \n",
    "        p_value_right = 1 - rv.cdf(test_stat)\n",
    "        p_value_left = rv.cdf(test_stat)\n",
    "        \n",
    "        # Two-sided test\n",
    "        if self.alternative == 'two-sided':\n",
    "            if (p_value_right < self.alpha/2) or (p_value_left < self.alpha/2):\n",
    "                self.pvalue_count+= 1\n",
    "        \n",
    "        # One-sided test using the right side of the distribution\n",
    "        elif self.alternative == 'right': \n",
    "            if p_value_right < self.alpha:\n",
    "                self.pvalue_count += 1\n",
    "        \n",
    "        # One-sided test using the left side of the distribution\n",
    "        elif self.alternative == 'left': \n",
    "            if p_value_left < self.alpha:\n",
    "                self.pvalue_count += 1\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"alternative has to be 'two-sided', 'right', or 'left\")\n",
    "    \n",
    "    # Method for computing power \n",
    "    def Power(self):\n",
    "        self.pvalue_count = 0\n",
    "        for i in range(self.num_runs):\n",
    "            self._RunPvalueCount()\n",
    "            \n",
    "        return self.pvalue_count / self.num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTCorrelationH0(PowerTest):\n",
    "    def __init__(self, data, alpha=0.05, alternative='two-sided', num_runs=100, method='pearson'):\n",
    "        PowerTest.__init__(self, data, alpha, alternative, num_runs)\n",
    "        self.method = method\n",
    "    \n",
    "    def PrepareData(self):\n",
    "        self.x, self.y = self.data\n",
    "        self.x = np.array(self.x)\n",
    "        self.y = np.array(self.y)\n",
    "        self.df = pd.DataFrame({'x':self.x, 'y': self.y})\n",
    "    \n",
    "    def ComputeRVandTestStat(self):\n",
    "        # Create run data\n",
    "        run_data = self.df.sample(n=len(self.df), replace=True)\n",
    "        run_x = run_data.x.values\n",
    "        run_y = run_data.y.values\n",
    "        \n",
    "        corrs=[]\n",
    "        \n",
    "        # Compute test_stat and build rv for the run\n",
    "        if self.method == 'pearson':\n",
    "            test_stat = stats.pearsonr(run_x , run_y)[0]\n",
    "            \n",
    "            for _ in range(100):\n",
    "                x_perm = np.random.permutation(run_x)\n",
    "                r = stats.pearsonr(x_perm , run_y)[0]\n",
    "                corrs.append(r)\n",
    "    \n",
    "        elif self.method == 'spearman':\n",
    "            test_stat = stats.spearmanr(run_x , run_y)[0]\n",
    "            \n",
    "            for _ in range(100):\n",
    "                x_perm = np.random.permutation(run_x)\n",
    "                r = stats.spearmanr(x_perm , run_y)[0]\n",
    "                corrs.append(r)\n",
    "    \n",
    "        else:\n",
    "            raise Exception('Must enter either pearson or spearman as a string for method argument')   \n",
    "        \n",
    "        rv = dsa.DiscreteRv(corrs)\n",
    "        \n",
    "        return test_stat, rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "      <th>abbrev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>18.8</td>\n",
       "      <td>7.332</td>\n",
       "      <td>5.640</td>\n",
       "      <td>18.048</td>\n",
       "      <td>15.040</td>\n",
       "      <td>784.55</td>\n",
       "      <td>145.08</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>18.1</td>\n",
       "      <td>7.421</td>\n",
       "      <td>4.525</td>\n",
       "      <td>16.290</td>\n",
       "      <td>17.014</td>\n",
       "      <td>1053.48</td>\n",
       "      <td>133.93</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>18.6</td>\n",
       "      <td>6.510</td>\n",
       "      <td>5.208</td>\n",
       "      <td>15.624</td>\n",
       "      <td>17.856</td>\n",
       "      <td>899.47</td>\n",
       "      <td>110.35</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total  speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "0   18.8     7.332    5.640          18.048       15.040       784.55   \n",
       "1   18.1     7.421    4.525          16.290       17.014      1053.48   \n",
       "2   18.6     6.510    5.208          15.624       17.856       899.47   \n",
       "\n",
       "   ins_losses abbrev  \n",
       "0      145.08     AL  \n",
       "1      133.93     AK  \n",
       "2      110.35     AZ  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car = sns.load_dataset('car_crashes')\n",
    "car.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.15689520004339752, 0.2715478689798989)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.pearsonr(car.no_previous, car.ins_premium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I think it might be working, a lot of variability in results though (0.17~0.35)\n",
    "# This variability is likely due to low sample size of just 51 for car data set\n",
    "# Gpower gives 0.19 for result\n",
    "data = car.no_previous, car.ins_premium\n",
    "car_power = PTCorrelationH0(data, alternative = 'two-sided')\n",
    "car_power.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: for the alternative hypothesis power test I think I do need a test stat variable in my class\n",
    "# Need to be able to provide zero or another value for null test stat\n",
    "# For null hypothesis power test I can calcuate the test-stat from the run data\n",
    "# Solved this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTCorrelationHa(PowerTest):\n",
    "    def __init__(self, data, test_stat, alpha=0.05, alternative='two-sided', num_runs=100, method='pearson'):\n",
    "        PowerTest.__init__(self, data, alpha, alternative, num_runs)\n",
    "        self.method = method\n",
    "        # Alternative hypothesis power tests require a test_stat be provided for null hypothesis (eg. zero for no effect)\n",
    "        self.test_stat = test_stat \n",
    "    \n",
    "    def PrepareData(self):\n",
    "        self.x, self.y = self.data\n",
    "        self.df = pd.DataFrame({'x':self.x, 'y': self.y})\n",
    "    \n",
    "    def ComputeRVandTestStat(self):\n",
    "        # Create run data\n",
    "        run_data = self.df.sample(n=len(self.df), replace=True)\n",
    "        \n",
    "        corrs=[]\n",
    "        \n",
    "        # Build rv\n",
    "        if self.method == 'pearson':          \n",
    "            for _ in range(100):\n",
    "                sample = run_data.sample(n=len(run_data), replace=True)\n",
    "                r = stats.pearsonr(sample.x, sample.y)[0]\n",
    "                corrs.append(r)\n",
    "    \n",
    "        elif self.method == 'spearman':            \n",
    "            for _ in range(100):\n",
    "                sample = run_data.sample(n=len(run_data), replace=True)\n",
    "                r = stats.spearmanr(sample.x, sample.y)[0]\n",
    "                corrs.append(r)\n",
    "    \n",
    "        else:\n",
    "            raise Exception('Must enter either pearson or spearman as a string for method argument')\n",
    "               \n",
    "        test_stat = self.test_stat\n",
    "        rv = dsa.DiscreteRv(corrs)\n",
    "        \n",
    "        return test_stat, rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This one seems to be working too but giving a bit lower values than null version 0.11~0.23\n",
    "data2 = car.no_previous, car.ins_premium\n",
    "car_power2 = PTCorrelationHa(data2, 0, alternative = 'two-sided')\n",
    "car_power2.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next create PTMeans, PTDiffMeansH0, and PTDiffMeansHa\n",
    "# Test all cases for correlation (eg. spearman too), and some edge cases like entering different length sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTMean(PowerTest):\n",
    "    def __init__(self, data, test_stat, alpha=0.05, alternative='two-sided', num_runs=100):\n",
    "        PowerTest.__init__(self, data, alpha, alternative, num_runs)\n",
    "        # Alternative hypothesis power tests require a test_stat be provided for null hypothesis (eg. zero for no effect)\n",
    "        self.test_stat = test_stat \n",
    "    \n",
    "    def PrepareData(self):\n",
    "        self.data = np.array(self.data)\n",
    "    \n",
    "    def ComputeRVandTestStat(self):\n",
    "        run_data = np.random.choice(self.data, size=len(self.data), replace=True)\n",
    "        mean_estimates = [np.random.choice(run_data, size=len(run_data), replace=True).mean() for _ in range(100)]\n",
    "        \n",
    "        test_stat = self.test_stat\n",
    "        rv = dsa.DiscreteRv(mean_estimates)\n",
    "        \n",
    "        return test_stat, rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_data = np.random.randint(-8,11,size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "powmean = PTMean(mean_data, 0, alternative='two-sided')\n",
    "powmean.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTDiffMeansHa(PowerTest):\n",
    "    def __init__(self, data, test_stat, alpha=0.05, alternative='two-sided', num_runs=100):\n",
    "        PowerTest.__init__(self, data, alpha, alternative, num_runs)\n",
    "        # Alternative hypothesis power tests require a test_stat be provided for null hypothesis (eg. zero for no effect)\n",
    "        self.test_stat = test_stat \n",
    "    \n",
    "    def PrepareData(self):\n",
    "        self.a, self.b = self.data\n",
    "        self.a = np.array(self.a)\n",
    "        self.b = np.array(self.b)\n",
    "    \n",
    "    def ComputeRVandTestStat(self):\n",
    "        # Create run data\n",
    "        sample1 = np.random.choice(self.a, size=len(self.a), replace=True)\n",
    "        sample2 = np.random.choice(self.b, size=len(self.b), replace=True)\n",
    "        \n",
    "        diff_mean_results = []\n",
    "        \n",
    "        # Build a sampling distribution for the run\n",
    "        for j in range(100):\n",
    "            group1 = np.random.choice(sample1, size=len(sample1), replace=True)\n",
    "            group2 = np.random.choice(sample2, size=len(sample2), replace=True)\n",
    "            result = group1.mean() - group2.mean()\n",
    "            diff_mean_results.append(result)\n",
    "        \n",
    "        test_stat = self.test_stat\n",
    "        rv = dsa.DiscreteRv(diff_mean_results)\n",
    "        \n",
    "        return test_stat, rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import first\n",
    "live, firsts, others = first.MakeFrames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [firsts.prglngth.values, others.prglngth.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffmeans = PTDiffMeansHa(data, 0)\n",
    "diffmeans.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTDiffMeansH0(PowerTest):\n",
    "    \n",
    "    def PrepareData(self):\n",
    "        self.a, self.b = self.data\n",
    "        self.a = np.array(self.a)\n",
    "        self.b = np.array(self.b)\n",
    "        self.pooled_data = np.hstack((self.a, self.b))\n",
    "        self.a_size = len(self.a)\n",
    "    \n",
    "    def ComputeRVandTestStat(self):\n",
    "        # Create run data by resampling the two groups\n",
    "        sample1 = np.random.choice(self.a, size=len(self.a), replace=True)\n",
    "        sample2 = np.random.choice(self.b, size=len(self.b), replace=True)\n",
    "        \n",
    "        # Calculate test_stat for the run data\n",
    "        test_stat = sample1.mean() - sample2.mean()\n",
    "        \n",
    "        diff_mean_results = []\n",
    "        \n",
    "        # Build a sampling distribution for the run\n",
    "        for j in range(100):\n",
    "            np.random.shuffle(self.pooled_data)\n",
    "            group1 = self.pooled_data[:self.a_size]\n",
    "            group2 = self.pooled_data[self.a_size:]\n",
    "            result = group1.mean() - group2.mean()\n",
    "            diff_mean_results.append(result)\n",
    "        \n",
    "        rv = dsa.DiscreteRv(diff_mean_results)\n",
    "        \n",
    "        return test_stat, rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [firsts.prglngth.values, others.prglngth.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I think both these diff means classes are working now!\n",
    "diffmeans2 = PTDiffMeansH0(data2)\n",
    "diffmeans2.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a chi-square power test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First play around to recall how my chi square function works\n",
    "# Could also look again at Star Trek example to recall the type of situation to use this in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = [11,7,8,9,10,12,13]\n",
    "expected = [10,10,10,10,10,10,10]\n",
    "observed = np.array(observed)\n",
    "expected = np.array(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70,\n",
       " [0, 1, 2, 3, 4, 5, 6],\n",
       " array([0.14285714, 0.14285714, 0.14285714, 0.14285714, 0.14285714,\n",
       "        0.14285714, 0.14285714]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = sum(expected)\n",
    "values = list(range(len(expected)))\n",
    "p_exp = expected/sum(expected)\n",
    "n, values, p_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = Counter({x:0 for x in values})\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 16, 1: 13, 2: 11, 3: 6, 4: 9, 5: 9, 6: 6})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.update(np.random.choice(values, size=n, replace=True, p=p_exp))\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 16), (1, 13), (2, 11), (3, 6), (4, 9), (5, 9), (6, 6)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_hist = sorted(hist.items())\n",
    "sorted_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16, 13, 11,  6,  9,  9,  6])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_observed = np.array([x[1] for x in sorted_hist])\n",
    "model_observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need a resampling of the population assuming the alternative hypothesis is true\n",
    "# Could I use the same kind of computation as done above for the null to simulate the alternative hypothesis?\n",
    "# Just use observed instead and create a new resampled sequence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70,\n",
       " [0, 1, 2, 3, 4, 5, 6],\n",
       " array([0.15714286, 0.1       , 0.11428571, 0.12857143, 0.14285714,\n",
       "        0.17142857, 0.18571429]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = sum(observed)\n",
    "values_obs = list(range(len(observed)))\n",
    "p_obs = observed/sum(observed)\n",
    "n, values_obs, p_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = Counter({x:0 for x in values_obs})\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 11, 1: 6, 2: 10, 3: 12, 4: 7, 5: 10, 6: 14})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.update(np.random.choice(values_obs, size=n, replace=True, p=p_obs))\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 11), (1, 6), (2, 10), (3, 12), (4, 7), (5, 10), (6, 14)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_hist_obs = sorted(hist.items())\n",
    "sorted_hist_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11,  6, 10, 12,  7, 10, 14])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I think this will work\n",
    "model_observed_obs = np.array([x[1] for x in sorted_hist_obs])\n",
    "model_observed_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Think about changing the docstring for my ResampleMean, and alternative hypothesis functions in my module\n",
    "# Is doubling the \"one-sided\" p-value really the right way to think about this?\n",
    "# Rather than doubling the p-value wouldn't it make more sense to halve the alpha when testing for significance?\n",
    "# This is the data we have and the p-value is what it is giving us\n",
    "# Try testing some cases in which the sampling distributions will be all positive or all negative\n",
    "# In this kind of case the p-values calculated should be the same regardless of the alternative used\n",
    "# Stats exchange discussions say \"don't halve the p-value\" but they are using only analytical methods\n",
    "# https://stats.stackexchange.com/questions/267192/doubling-or-halving-p-values-for-one-vs-two-tailed-tests\n",
    "\n",
    "# Also try comparing results between Ha and H0 hypothesis tests\n",
    "# to see whether or not doubling p-values from Ha distributions is comparable to two-sided H0 results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
