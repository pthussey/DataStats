{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime as dt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import dataStatsAnalysis as dsa\n",
    "import dataStatsPlotting as dsp\n",
    "\n",
    "dsp.SetParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerTest():\n",
    "    \n",
    "    def __init__(self, data, alpha=0.05, alternative='two-sided', num_runs=100):\n",
    "        self.data = data\n",
    "        self.alpha = alpha\n",
    "        self.alternative = alternative\n",
    "        self.num_runs = num_runs\n",
    "        self.PrepareData()\n",
    "    \n",
    "    # Provide functionality to convert the data into format needed for use in BuildRv\n",
    "    # Ex. Convert to array, split data into component groups, etc.\n",
    "    # See child classes for examples\n",
    "    def PrepareData(self):\n",
    "        UnimplementedMethodException()\n",
    "    \n",
    "    # Provide functionality that creates the run data and then computes the run's test stat and rv\n",
    "    # This involves doing one resample to simulate pulling an additional sample from the population,\n",
    "    # then calculating the test_stat, building a sampling distribution, and computing the rv\n",
    "    # See child classes for examples\n",
    "    def ComputeRVandTestStat(self):\n",
    "        UnimplementedMethodException()\n",
    "    \n",
    "    # Computes the pvalue of test stat from an rv,\n",
    "    # and adds to pvalue_count if less than significance level\n",
    "    def _RunPvalueCount(self):\n",
    "        test_stat, rv = self.ComputeRVandTestStat()\n",
    "        \n",
    "        p_value_right = 1 - rv.cdf(test_stat)\n",
    "        p_value_left = rv.cdf(test_stat)\n",
    "        \n",
    "        # Two-sided test\n",
    "        if self.alternative == 'two-sided':\n",
    "            if (p_value_right < self.alpha/2) or (p_value_left < self.alpha/2):\n",
    "                self.pvalue_count+= 1\n",
    "        \n",
    "        # One-sided test using the right side of the distribution\n",
    "        elif self.alternative == 'right': \n",
    "            if p_value_right < self.alpha:\n",
    "                self.pvalue_count += 1\n",
    "        \n",
    "        # One-sided test using the left side of the distribution\n",
    "        elif self.alternative == 'left': \n",
    "            if p_value_left < self.alpha:\n",
    "                self.pvalue_count += 1\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"alternative has to be 'two-sided', 'right', or 'left\")\n",
    "    \n",
    "    # Method for computing power \n",
    "    def Power(self):\n",
    "        self.pvalue_count = 0\n",
    "        for i in range(self.num_runs):\n",
    "            self._RunPvalueCount()\n",
    "            \n",
    "        return self.pvalue_count / self.num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTCorrelationH0(PowerTest):\n",
    "    def __init__(self, data, alpha=0.05, alternative='two-sided', num_runs=100, method='pearson'):\n",
    "        PowerTest.__init__(self, data, alpha, alternative, num_runs)\n",
    "        self.method = method\n",
    "    \n",
    "    def PrepareData(self):\n",
    "        self.x, self.y = self.data\n",
    "        self.x = np.array(self.x)\n",
    "        self.y = np.array(self.y)\n",
    "        self.df = pd.DataFrame({'x':self.x, 'y': self.y})\n",
    "    \n",
    "    def ComputeRVandTestStat(self):\n",
    "        # Create run data\n",
    "        run_data = self.df.sample(n=len(self.df), replace=True)\n",
    "        run_x = run_data.x.values\n",
    "        run_y = run_data.y.values\n",
    "        \n",
    "        corrs=[]\n",
    "        \n",
    "        # Compute test_stat and build rv for the run\n",
    "        if self.method == 'pearson':\n",
    "            test_stat = stats.pearsonr(run_x , run_y)[0]\n",
    "            \n",
    "            for _ in range(100):\n",
    "                x_perm = np.random.permutation(run_x)\n",
    "                r = stats.pearsonr(x_perm , run_y)[0]\n",
    "                corrs.append(r)\n",
    "    \n",
    "        elif self.method == 'spearman':\n",
    "            test_stat = stats.spearmanr(run_x , run_y)[0]\n",
    "            \n",
    "            for _ in range(100):\n",
    "                x_perm = np.random.permutation(run_x)\n",
    "                r = stats.spearmanr(x_perm , run_y)[0]\n",
    "                corrs.append(r)\n",
    "    \n",
    "        else:\n",
    "            raise Exception('Must enter either pearson or spearman as a string for method argument')   \n",
    "        \n",
    "        rv = dsa.DiscreteRv(corrs)\n",
    "        \n",
    "        return test_stat, rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>speeding</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>not_distracted</th>\n",
       "      <th>no_previous</th>\n",
       "      <th>ins_premium</th>\n",
       "      <th>ins_losses</th>\n",
       "      <th>abbrev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.8</td>\n",
       "      <td>7.332</td>\n",
       "      <td>5.640</td>\n",
       "      <td>18.048</td>\n",
       "      <td>15.040</td>\n",
       "      <td>784.55</td>\n",
       "      <td>145.08</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.1</td>\n",
       "      <td>7.421</td>\n",
       "      <td>4.525</td>\n",
       "      <td>16.290</td>\n",
       "      <td>17.014</td>\n",
       "      <td>1053.48</td>\n",
       "      <td>133.93</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.6</td>\n",
       "      <td>6.510</td>\n",
       "      <td>5.208</td>\n",
       "      <td>15.624</td>\n",
       "      <td>17.856</td>\n",
       "      <td>899.47</td>\n",
       "      <td>110.35</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total  speeding  alcohol  not_distracted  no_previous  ins_premium  \\\n",
       "0   18.8     7.332    5.640          18.048       15.040       784.55   \n",
       "1   18.1     7.421    4.525          16.290       17.014      1053.48   \n",
       "2   18.6     6.510    5.208          15.624       17.856       899.47   \n",
       "\n",
       "   ins_losses abbrev  \n",
       "0      145.08     AL  \n",
       "1      133.93     AK  \n",
       "2      110.35     AZ  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car = sns.load_dataset('car_crashes')\n",
    "car.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.1568952000433975, 0.2715478689798987)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.pearsonr(car.no_previous, car.ins_premium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I think it might be working, a lot of variability in results though (0.17~0.35)\n",
    "# This variability is likely due to low sample size of just 51 for car data set\n",
    "# Gpower gives 0.19 for result\n",
    "data = car.no_previous, car.ins_premium\n",
    "car_power = PTCorrelationH0(data, alternative = 'two-sided', method='pearson')\n",
    "car_power.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: for the alternative hypothesis power test I think I do need a test stat variable in my class\n",
    "# Need to be able to provide zero or another value for null test stat\n",
    "# For null hypothesis power test I can calcuate the test-stat from the run data\n",
    "# Solved this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTCorrelationHa(PowerTest):\n",
    "    def __init__(self, data, test_stat, alpha=0.05, alternative='two-sided', num_runs=100, method='pearson'):\n",
    "        PowerTest.__init__(self, data, alpha, alternative, num_runs)\n",
    "        self.method = method\n",
    "        # Alternative hypothesis power tests require a test_stat be provided for null hypothesis (eg. zero for no effect)\n",
    "        self.test_stat = test_stat \n",
    "    \n",
    "    def PrepareData(self):\n",
    "        self.x, self.y = self.data\n",
    "        self.df = pd.DataFrame({'x':self.x, 'y': self.y})\n",
    "    \n",
    "    def ComputeRVandTestStat(self):\n",
    "        # Create run data\n",
    "        run_data = self.df.sample(n=len(self.df), replace=True)\n",
    "        \n",
    "        corrs=[]\n",
    "        \n",
    "        # Build rv\n",
    "        if self.method == 'pearson':          \n",
    "            for _ in range(100):\n",
    "                sample = run_data.sample(n=len(run_data), replace=True)\n",
    "                r = stats.pearsonr(sample.x, sample.y)[0]\n",
    "                corrs.append(r)\n",
    "    \n",
    "        elif self.method == 'spearman':            \n",
    "            for _ in range(100):\n",
    "                sample = run_data.sample(n=len(run_data), replace=True)\n",
    "                r = stats.spearmanr(sample.x, sample.y)[0]\n",
    "                corrs.append(r)\n",
    "    \n",
    "        else:\n",
    "            raise Exception('Must enter either pearson or spearman as a string for method argument')\n",
    "               \n",
    "        test_stat = self.test_stat\n",
    "        rv = dsa.DiscreteRv(corrs)\n",
    "        \n",
    "        return test_stat, rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This one seems to be working too but giving a bit lower values than null version 0.11~0.23\n",
    "data2 = car.no_previous, car.ins_premium\n",
    "car_power2 = PTCorrelationHa(data2, 0, alternative = 'two-sided', method='pearson')\n",
    "car_power2.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next create PTMeans, PTDiffMeansH0, and PTDiffMeansHa - done\n",
    "# Test all cases for correlation (eg. spearman too), and some edge cases like entering different length sequences\n",
    "# Spearman works, giving lower power than pearson\n",
    "# Different length sequences produce ValueError: arrays must all be same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case of different sequence lengths\n",
    "# Produces ValueError: arrays must all be same length, as expected\n",
    "# s1 = np.random.randint(1,50,100)\n",
    "# s2 = np.random.randint(1,50,90)\n",
    "# data3 = s1, s2\n",
    "# test_power = PTCorrelationHa(data3, 0, alternative = 'two-sided', method='pearson')\n",
    "# test_power.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTMean(PowerTest):\n",
    "    def __init__(self, data, test_stat, alpha=0.05, alternative='two-sided', num_runs=100):\n",
    "        PowerTest.__init__(self, data, alpha, alternative, num_runs)\n",
    "        # Alternative hypothesis power tests require a test_stat be provided for null hypothesis (eg. zero for no effect)\n",
    "        self.test_stat = test_stat \n",
    "    \n",
    "    def PrepareData(self):\n",
    "        self.data = np.array(self.data)\n",
    "    \n",
    "    def ComputeRVandTestStat(self):\n",
    "        run_data = np.random.choice(self.data, size=len(self.data), replace=True)\n",
    "        mean_estimates = [np.random.choice(run_data, size=len(run_data), replace=True).mean() for _ in range(100)]\n",
    "        \n",
    "        test_stat = self.test_stat\n",
    "        rv = dsa.DiscreteRv(mean_estimates)\n",
    "        \n",
    "        return test_stat, rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_data = np.random.randint(-8,11,size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "powmean = PTMean(mean_data, 0, alternative='two-sided')\n",
    "powmean.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTDiffMeansHa(PowerTest):\n",
    "    def __init__(self, data, test_stat, alpha=0.05, alternative='two-sided', num_runs=100):\n",
    "        PowerTest.__init__(self, data, alpha, alternative, num_runs)\n",
    "        # Alternative hypothesis power tests require a test_stat be provided for null hypothesis (eg. zero for no effect)\n",
    "        self.test_stat = test_stat \n",
    "    \n",
    "    def PrepareData(self):\n",
    "        self.a, self.b = self.data\n",
    "        self.a = np.array(self.a)\n",
    "        self.b = np.array(self.b)\n",
    "    \n",
    "    def ComputeRVandTestStat(self):\n",
    "        # Create run data\n",
    "        sample1 = np.random.choice(self.a, size=len(self.a), replace=True)\n",
    "        sample2 = np.random.choice(self.b, size=len(self.b), replace=True)\n",
    "        \n",
    "        diff_mean_results = []\n",
    "        \n",
    "        # Build a sampling distribution for the run\n",
    "        for _ in range(100):\n",
    "            group1 = np.random.choice(sample1, size=len(sample1), replace=True)\n",
    "            group2 = np.random.choice(sample2, size=len(sample2), replace=True)\n",
    "            result = group1.mean() - group2.mean()\n",
    "            diff_mean_results.append(result)\n",
    "        \n",
    "        test_stat = self.test_stat\n",
    "        rv = dsa.DiscreteRv(diff_mean_results)\n",
    "        \n",
    "        return test_stat, rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import first\n",
    "live, firsts, others = first.MakeFrames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [firsts.prglngth.values, others.prglngth.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffmeans = PTDiffMeansHa(data, 0, num_runs=100)\n",
    "diffmeans.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTDiffMeansH0(PowerTest):\n",
    "    \n",
    "    def PrepareData(self):\n",
    "        self.a, self.b = self.data\n",
    "        self.a = np.array(self.a)\n",
    "        self.b = np.array(self.b)\n",
    "        self.pooled_data = np.hstack((self.a, self.b))\n",
    "        self.a_size = len(self.a)\n",
    "    \n",
    "    def ComputeRVandTestStat(self):\n",
    "        # Create run data by resampling the two groups\n",
    "        sample1 = np.random.choice(self.a, size=len(self.a), replace=True)\n",
    "        sample2 = np.random.choice(self.b, size=len(self.b), replace=True)\n",
    "        \n",
    "        # Calculate test_stat for the run data\n",
    "        test_stat = sample1.mean() - sample2.mean()\n",
    "        \n",
    "        diff_mean_results = []\n",
    "        \n",
    "        # Build a sampling distribution for the run\n",
    "        for _ in range(100):\n",
    "            np.random.shuffle(self.pooled_data)\n",
    "            group1 = self.pooled_data[:self.a_size]\n",
    "            group2 = self.pooled_data[self.a_size:]\n",
    "            result = group1.mean() - group2.mean()\n",
    "            diff_mean_results.append(result)\n",
    "        \n",
    "        rv = dsa.DiscreteRv(diff_mean_results)\n",
    "        \n",
    "        return test_stat, rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [firsts.prglngth.values, others.prglngth.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I think both these diff means classes are working now!\n",
    "diffmeans2 = PTDiffMeansH0(data2, num_runs=100)\n",
    "diffmeans2.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a chi-square power test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First play around to recall how my chi square function works\n",
    "# Could also look again at Star Trek example to recall the type of situation to use this in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = [11,7,4,7,12,16,13]\n",
    "expected = [10,10,10,10,10,10,10]\n",
    "observed = np.array(observed)\n",
    "expected = np.array(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70,\n",
       " [0, 1, 2, 3, 4, 5, 6],\n",
       " array([0.14285714, 0.14285714, 0.14285714, 0.14285714, 0.14285714,\n",
       "        0.14285714, 0.14285714]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = sum(expected)\n",
    "values = list(range(len(expected)))\n",
    "p_exp = expected/sum(expected)\n",
    "n, values, p_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = Counter({x:0 for x in values})\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 8, 1: 7, 2: 8, 3: 13, 4: 11, 5: 13, 6: 10})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.update(np.random.choice(values, size=n, replace=True, p=p_exp))\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 8), (1, 7), (2, 8), (3, 13), (4, 11), (5, 13), (6, 10)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_hist = sorted(hist.items())\n",
    "sorted_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  7,  8, 13, 11, 13, 10])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_observed = np.array([x[1] for x in sorted_hist])\n",
    "model_observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need a resampling of the population assuming the alternative hypothesis is true\n",
    "# Could I use the same kind of computation as done above for the null to simulate the alternative hypothesis?\n",
    "# Just use observed instead and create a new resampled sequence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70,\n",
       " [0, 1, 2, 3, 4, 5, 6],\n",
       " array([0.15714286, 0.1       , 0.05714286, 0.1       , 0.17142857,\n",
       "        0.22857143, 0.18571429]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = sum(observed)\n",
    "values_obs = list(range(len(observed)))\n",
    "p_obs = observed/sum(observed)\n",
    "n, values_obs, p_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = Counter({x:0 for x in values_obs})\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 8, 1: 1, 2: 4, 3: 10, 4: 14, 5: 20, 6: 13})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.update(np.random.choice(values_obs, size=n, replace=True, p=p_obs))\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 8), (1, 1), (2, 4), (3, 10), (4, 14), (5, 20), (6, 13)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_hist_obs = sorted(hist.items())\n",
    "sorted_hist_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  1,  4, 10, 14, 20, 13])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I think this will work\n",
    "model_observed_obs = np.array([x[1] for x in sorted_hist_obs])\n",
    "model_observed_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTChiSquareH0(PowerTest):\n",
    "    '''Takes data in form of two sequences: data = observed, expected'''\n",
    "    \n",
    "    def PrepareData(self):\n",
    "        self.observed, self.expected = self.data\n",
    "        self.observed = np.array(self.observed)\n",
    "        self.expected = np.array(self.expected)\n",
    "    \n",
    "    def ComputeRVandTestStat(self):\n",
    "        # Create run data by resampling the observed sequence (assuming the alternative hypothesis)\n",
    "        n = sum(self.observed)\n",
    "        values_obs = list(range(len(self.observed)))\n",
    "        p_obs = self.observed/sum(self.observed)\n",
    "        \n",
    "        hist = Counter({x:0 for x in values_obs})\n",
    "        hist.update(np.random.choice(values_obs, size=n, replace=True, p=p_obs))\n",
    "        sorted_hist = sorted(hist.items())\n",
    "        run_observed = np.array([x[1] for x in sorted_hist])\n",
    "        \n",
    "        # Calculate test_stat for the run data using the observed sequence (alternative hypothesis)\n",
    "        test_stat = sum((run_observed - self.expected)**2 / self.expected)\n",
    "        \n",
    "        chis = []\n",
    "        \n",
    "        # Build a chi sqaure sampling distribution for the run using the expected sequence (null hypothesis)\n",
    "        for _ in range(100):\n",
    "            n = sum(self.expected)\n",
    "            values = list(range(len(self.expected)))\n",
    "            p_exp = self.expected/sum(self.expected)\n",
    "            \n",
    "            hist = Counter({x:0 for x in values}) # Initialize a Counter with zero values\n",
    "            hist.update(np.random.choice(values, size=n, replace=True, p=p_exp))\n",
    "            sorted_hist = sorted(hist.items())\n",
    "            model_observed = np.array([x[1] for x in sorted_hist])\n",
    "            chi = sum((model_observed - self.expected)**2 / self.expected)\n",
    "            chis.append(chi)\n",
    "        \n",
    "        rv = dsa.DiscreteRv(chis)\n",
    "        \n",
    "        return test_stat, rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = [11,7,4,7,12,16,13]\n",
    "expected = [10,10,10,10,10,10,10]\n",
    "observed = np.array(observed)\n",
    "expected = np.array(expected)\n",
    "data = observed, expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these results are 0.52~0.66\n",
    "ptchi = PTChiSquareH0(data)\n",
    "ptchi.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3854496446637727"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try with statsmodels\n",
    "p_exp = expected/sum(expected)\n",
    "p_obs = observed/sum(observed)\n",
    "effect = sms.gof.chisquare_effectsize(p_exp, p_obs)\n",
    "effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6642702634182477"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not entirely sure what nobs and n_bins are\n",
    "# I think I've figured out what nobs and n_bins are\n",
    "# And now the results match very well with mine\n",
    "# nobs is the total number of observations (ie. sum(observed) or sum(expected))\n",
    "# n_bins is the number of cells in the sequence/array (ie. len(observed) or len(expected))\n",
    "smsptchi = sms.GofChisquarePower()\n",
    "smsptchi.solve_power(effect_size=effect, nobs=70, alpha=0.05, n_bins=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an alternative hypothesis chi square power test too\n",
    "# I've decided not to include this in my dsa module\n",
    "# The main reason is that I don't have confidence in the methodology of calculating the test_stat\n",
    "# It seems odd to calculate the test_stat using a run_observed calculated from the expected values\n",
    "# This could be valid but I'm just not sure\n",
    "# Plus having two of these functions is really overkill I think\n",
    "class PTChiSquareHa(PowerTest):\n",
    "    '''Takes data in form of two sequences: data = observed, expected'''\n",
    "    \n",
    "    def PrepareData(self):\n",
    "        self.observed, self.expected = self.data\n",
    "        self.observed = np.array(self.observed)\n",
    "        self.expected = np.array(self.expected)\n",
    "    \n",
    "    def ComputeRVandTestStat(self):\n",
    "        # Create run data by resampling the expected sequence (assuming the null hypothesis)\n",
    "        n = sum(self.expected)\n",
    "        values_exp = list(range(len(self.expected)))\n",
    "        p_exp = self.expected/sum(self.expected)\n",
    "        \n",
    "        hist = Counter({x:0 for x in values_exp})\n",
    "        hist.update(np.random.choice(values_exp, size=n, replace=True, p=p_exp))\n",
    "        sorted_hist = sorted(hist.items())\n",
    "        run_observed = np.array([x[1] for x in sorted_hist])\n",
    "        \n",
    "        # Calculate test_stat for the run data (assuming the null hypothesis)\n",
    "        test_stat = sum((run_observed - self.expected)**2 / self.expected)\n",
    "        \n",
    "        chis = []\n",
    "        \n",
    "        # Build a chi square sampling distribution for the run using the observed sequence (alternative hypothesis)\n",
    "        for _ in range(100):\n",
    "            n = sum(self.observed)\n",
    "            values = list(range(len(self.observed)))\n",
    "            p_obs = self.observed/sum(self.observed)\n",
    "            \n",
    "            hist = Counter({x:0 for x in values}) # Initialize a Counter with zero values\n",
    "            hist.update(np.random.choice(values, size=n, replace=True, p=p_obs))\n",
    "            sorted_hist = sorted(hist.items())\n",
    "            model_observed = np.array([x[1] for x in sorted_hist])\n",
    "            chi = sum((model_observed - self.expected)**2 / self.expected)\n",
    "            chis.append(chi)\n",
    "        \n",
    "        rv = dsa.DiscreteRv(chis)\n",
    "        \n",
    "        return test_stat, rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = [11,7,4,7,12,16,13]\n",
    "expected = [10,10,10,10,10,10,10]\n",
    "observed = np.array(observed)\n",
    "expected = np.array(expected)\n",
    "data2 = observed, expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is 0.43~0.54 (lower than H0 version)\n",
    "ptchi = PTChiSquareHa(data2)\n",
    "ptchi.Power()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Think about changing the docstring for my ResampleMean, and alternative hypothesis functions in my module\n",
    "# Is doubling the \"one-sided\" p-value really the right way to think about this?\n",
    "# Rather than doubling the p-value wouldn't it make more sense to halve the alpha when testing for significance?\n",
    "# This is the data we have and the p-value is what it is giving us\n",
    "# Try testing some cases in which the sampling distributions will be all positive or all negative\n",
    "# In this kind of case the p-values calculated should be the same regardless of the alternative used\n",
    "# Stats exchange discussions say \"don't halve the p-value\" but they are using only analytical methods\n",
    "# Also I'm talking about halving alpha, not changing the p-value\n",
    "# https://stats.stackexchange.com/questions/267192/doubling-or-halving-p-values-for-one-vs-two-tailed-tests\n",
    "# Try to find the discussion of this in the Statistics by Jim book\n",
    "\n",
    "# Also try comparing results between Ha and H0 hypothesis tests\n",
    "# to see whether or not doubling p-values from Ha distributions is comparable to two-sided H0 results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
